{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile EC_F_PES2UG23CS381_Lab3.py\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def get_entropy_of_dataset(data: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the entropy of the entire dataset using the target variable (last column).\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Dataset where the last column is the target variable\n",
        "\n",
        "    Returns:\n",
        "        float: Entropy value calculated using the formula:\n",
        "               Entropy = -Î£(p_i * log2(p_i)) where p_i is the probability of class i\n",
        "\n",
        "    Example:\n",
        "        data = np.array([[1, 0, 'yes'],\n",
        "                        [1, 1, 'no'],\n",
        "                        [0, 0, 'yes']])\n",
        "        entropy = get_entropy_of_dataset(data)\n",
        "        # Should return entropy based on target column ['yes', 'no', 'yes']\n",
        "    \"\"\"\n",
        "    target_column = data[:, -1]\n",
        "    class_counts = Counter(target_column)\n",
        "    total_samples = len(target_column)\n",
        "    entropy = 0.0\n",
        "\n",
        "    if total_samples == 0:\n",
        "        return 0.0\n",
        "\n",
        "    for count in class_counts.values():\n",
        "        probability = count / total_samples\n",
        "        if probability > 0:\n",
        "            entropy -= probability * math.log2(probability)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "    # TODO: Implement entropy calculation\n",
        "    # Hint: Use np.unique() to get unique classes and their counts\n",
        "    # Hint: Handle the case when probability is 0 to avoid log2(0)\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_avg_info_of_attribute(data: np.ndarray, attribute: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the average information (weighted entropy) of a specific attribute.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Dataset where the last column is the target variable\n",
        "        attribute (int): Index of the attribute column to calculate average information for\n",
        "\n",
        "    Returns:\n",
        "        float: Average information calculated using the formula:\n",
        "               Avg_Info = Î£((|S_v|/|S|) * Entropy(S_v))\n",
        "               where S_v is subset of data with attribute value v\n",
        "\n",
        "    Example:\n",
        "        data = np.array([[1, 0, 'yes'],\n",
        "                        [1, 1, 'no'],\n",
        "                        [0, 0, 'yes']])\n",
        "        avg_info = get_avg_info_of_attribute(data, 0)  # For attribute at index 0\n",
        "        # Should return weighted average entropy for attribute splits\n",
        "\n",
        "    \"\"\"\n",
        "    attribute_values = np.unique(data[:, attribute])\n",
        "    total_samples = len(data)\n",
        "    avg_info = 0.0\n",
        "\n",
        "    for value in attribute_values:\n",
        "        subset = data[data[:, attribute] == value]\n",
        "        subset_entropy = get_entropy_of_dataset(subset)\n",
        "        avg_info += (len(subset) / total_samples) * subset_entropy\n",
        "\n",
        "    return avg_info\n",
        "    # TODO: Implement average information calculation\n",
        "    # Hint: For each unique value in the attribute column:\n",
        "    #   1. Create a subset of data with that value\n",
        "    #   2. Calculate the entropy of that subset\n",
        "    #   3. Weight it by the proportion of samples with that value\n",
        "    #   4. Sum all weighted entropies\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_information_gain(data: np.ndarray, attribute: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the Information Gain for a specific attribute.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Dataset where the last column is the target variable\n",
        "        attribute (int): Index of the attribute column to calculate information gain for\n",
        "\n",
        "    Returns:\n",
        "        float: Information gain calculated using the formula:\n",
        "               Information_Gain = Entropy(S) - Avg_Info(attribute)\n",
        "               Rounded to 4 decimal places\n",
        "\n",
        "    Example:\n",
        "        data = np.array([[1, 0, 'yes'],\n",
        "                        [1, 1, 'no'],\n",
        "                        [0, 0, 'yes']])\n",
        "        gain = get_information_gain(data, 0)  # For attribute at index 0\n",
        "        # Should return the information gain for splitting on attribute 0\n",
        "    \"\"\"\n",
        "    dataset_entropy = get_entropy_of_dataset(data)\n",
        "    avg_info = get_avg_info_of_attribute(data, attribute)\n",
        "    information_gain = dataset_entropy - avg_info\n",
        "\n",
        "    return round(information_gain, 4)\n",
        "    # TODO: Implement information gain calculation\n",
        "    # Hint: Information Gain = Dataset Entropy - Average Information of Attribute\n",
        "    # Hint: Use the functions you implemented above\n",
        "    # Hint: Round the result to 4 decimal places\n",
        "    pass\n",
        "\n",
        "def get_selected_attribute(data: np.ndarray) -> tuple:\n",
        "    \"\"\"\n",
        "    Select the best attribute based on highest information gain.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Dataset where the last column is the target variable\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - dict: Dictionary mapping attribute indices to their information gains\n",
        "            - int: Index of the attribute with the highest information gain\n",
        "\n",
        "    Example:\n",
        "        data = np.array([[1, 0, 2, 'yes'],\n",
        "                        [1, 1, 1, 'no'],\n",
        "                        [0, 0, 2, 'yes']])\n",
        "        result = get_selected_attribute(data)\n",
        "        # Should return something like: ({0: 0.123, 1: 0.456, 2: 0.789}, 2)\n",
        "        # where 2 is the index of the attribute with highest gain\n",
        "    \"\"\"\n",
        "    num_attributes = data.shape[1] - 1\n",
        "    information_gains = {}\n",
        "\n",
        "    for attribute_index in range(num_attributes):\n",
        "        gain = get_information_gain(data, attribute_index)\n",
        "        information_gains[attribute_index] = gain\n",
        "\n",
        "    if not information_gains:\n",
        "        return ({}, -1)\n",
        "\n",
        "    selected_attribute = max(information_gains, key=information_gains.get)\n",
        "\n",
        "    return (information_gains, selected_attribute)\n",
        "\n",
        "    # TODO: Implement attribute selection\n",
        "    # Hint: Calculate information gain for all attributes (except target variable)\n",
        "    # Hint: Store gains in a dictionary with attribute index as key\n",
        "    # Hint: Find the attribute with maximum gain using max() with key parameter\n",
        "    # Hint: Return tuple (gain_dictionary, selected_attribute_index)\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6ylhVaRuuD",
        "outputId": "76f3aafe-7ffd-4103-9bbd-be1ddb5b5060"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting EC_F_PES2UG23CS381_Lab3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on Mushrooms dataset"
      ],
      "metadata": {
        "id": "Au_mG5hIWLqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --ID EC_F_PES2UG23CS381_Lab3 --data mushrooms.csv --framework sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4v4yGakVvFD",
        "outputId": "33ff6f50-dfe9-4d8e-cdcf-cf73216d95eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests with SKLEARN framework\n",
            "============================================================\n",
            " target column: 'class' (last column)\n",
            "Original dataset info:\n",
            "Shape: (8124, 23)\n",
            "Columns: ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat', 'class']\n",
            "\n",
            "First few rows:\n",
            "\n",
            "cap-shape: ['x' 'b' 's' 'f' 'k'] -> [5 0 4 2 3]\n",
            "\n",
            "cap-surface: ['s' 'y' 'f' 'g'] -> [2 3 0 1]\n",
            "\n",
            "cap-color: ['n' 'y' 'w' 'g' 'e'] -> [4 9 8 3 2]\n",
            "\n",
            "class: ['p' 'e'] -> [1 0]\n",
            "\n",
            "Processed dataset shape: (8124, 23)\n",
            "Number of features: 22\n",
            "Features: ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n",
            "Target: class\n",
            "Framework: SKLEARN\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "\n",
            "============================================================\n",
            "DECISION TREE CONSTRUCTION DEMO\n",
            "============================================================\n",
            "Total samples: 8124\n",
            "Training samples: 6499\n",
            "Testing samples: 1625\n",
            "\n",
            "Constructing decision tree using training data...\n",
            "\n",
            "ðŸŒ³ Decision tree construction completed using SKLEARN!\n",
            "\n",
            "ðŸ“Š OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             1.0000 (100.00%)\n",
            "Precision (weighted): 1.0000\n",
            "Recall (weighted):    1.0000\n",
            "F1-Score (weighted):  1.0000\n",
            "Precision (macro):    1.0000\n",
            "Recall (macro):       1.0000\n",
            "F1-Score (macro):     1.0000\n",
            "\n",
            "ðŸŒ³ TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        4\n",
            "Total Nodes:          29\n",
            "Leaf Nodes:           24\n",
            "Internal Nodes:       5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on TicTacToe dataset"
      ],
      "metadata": {
        "id": "dSXtI0H9WXZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --ID EC_F_PES2UG23CS381_Lab3 --data tictactoe.csv --framework sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7_5emRqVu_L",
        "outputId": "99e6f046-9934-46e2-e8ac-76519037f540"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests with SKLEARN framework\n",
            "============================================================\n",
            " target column: 'Class' (last column)\n",
            "Original dataset info:\n",
            "Shape: (958, 10)\n",
            "Columns: ['top-left-square', 'top-middle-square', 'top-right-square', 'middle-left-square', 'middle-middle-square', 'middle-right-square', 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square', 'Class']\n",
            "\n",
            "First few rows:\n",
            "\n",
            "top-left-square: ['x' 'o' 'b'] -> [2 1 0]\n",
            "\n",
            "top-middle-square: ['x' 'o' 'b'] -> [2 1 0]\n",
            "\n",
            "top-right-square: ['x' 'o' 'b'] -> [2 1 0]\n",
            "\n",
            "Class: ['positive' 'negative'] -> [1 0]\n",
            "\n",
            "Processed dataset shape: (958, 10)\n",
            "Number of features: 9\n",
            "Features: ['top-left-square', 'top-middle-square', 'top-right-square', 'middle-left-square', 'middle-middle-square', 'middle-right-square', 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square']\n",
            "Target: Class\n",
            "Framework: SKLEARN\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "\n",
            "============================================================\n",
            "DECISION TREE CONSTRUCTION DEMO\n",
            "============================================================\n",
            "Total samples: 958\n",
            "Training samples: 766\n",
            "Testing samples: 192\n",
            "\n",
            "Constructing decision tree using training data...\n",
            "\n",
            "ðŸŒ³ Decision tree construction completed using SKLEARN!\n",
            "\n",
            "ðŸ“Š OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             0.8836 (88.36%)\n",
            "Precision (weighted): 0.8827\n",
            "Recall (weighted):    0.8836\n",
            "F1-Score (weighted):  0.8822\n",
            "Precision (macro):    0.8784\n",
            "Recall (macro):       0.8600\n",
            "F1-Score (macro):     0.8680\n",
            "\n",
            "ðŸŒ³ TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        7\n",
            "Total Nodes:          260\n",
            "Leaf Nodes:           165\n",
            "Internal Nodes:       95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on Nursery dataset"
      ],
      "metadata": {
        "id": "iOqt5U86Wd0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --ID EC_F_PES2UG23CS381_Lab3 --data Nursery.csv --framework sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW5fByIDV9JU",
        "outputId": "ee4d6010-969f-4b2f-a544-28644c7abd7e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests with SKLEARN framework\n",
            "============================================================\n",
            " target column: 'class' (last column)\n",
            "Original dataset info:\n",
            "Shape: (12960, 9)\n",
            "Columns: ['parents', 'has_nurs', 'form', 'children', 'housing', 'finance', 'social', 'health', 'class']\n",
            "\n",
            "First few rows:\n",
            "\n",
            "parents: ['usual' 'pretentious' 'great_pret'] -> [2 1 0]\n",
            "\n",
            "has_nurs: ['proper' 'less_proper' 'improper' 'critical' 'very_crit'] -> [3 2 1 0 4]\n",
            "\n",
            "form: ['complete' 'completed' 'incomplete' 'foster'] -> [0 1 3 2]\n",
            "\n",
            "class: ['recommend' 'priority' 'not_recom' 'very_recom' 'spec_prior'] -> [2 1 0 4 3]\n",
            "\n",
            "Processed dataset shape: (12960, 9)\n",
            "Number of features: 8\n",
            "Features: ['parents', 'has_nurs', 'form', 'children', 'housing', 'finance', 'social', 'health']\n",
            "Target: class\n",
            "Framework: SKLEARN\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "\n",
            "============================================================\n",
            "DECISION TREE CONSTRUCTION DEMO\n",
            "============================================================\n",
            "Total samples: 12960\n",
            "Training samples: 10368\n",
            "Testing samples: 2592\n",
            "\n",
            "Constructing decision tree using training data...\n",
            "\n",
            "ðŸŒ³ Decision tree construction completed using SKLEARN!\n",
            "\n",
            "ðŸ“Š OVERALL PERFORMANCE METRICS\n",
            "========================================\n",
            "Accuracy:             0.9887 (98.87%)\n",
            "Precision (weighted): 0.9888\n",
            "Recall (weighted):    0.9887\n",
            "F1-Score (weighted):  0.9887\n",
            "Precision (macro):    0.9577\n",
            "Recall (macro):       0.9576\n",
            "F1-Score (macro):     0.9576\n",
            "\n",
            "ðŸŒ³ TREE COMPLEXITY METRICS\n",
            "========================================\n",
            "Maximum Depth:        7\n",
            "Total Nodes:          983\n",
            "Leaf Nodes:           703\n",
            "Internal Nodes:       280\n"
          ]
        }
      ]
    }
  ]
}
